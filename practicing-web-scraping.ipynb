{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with Beautiful Soup - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Now that you've read and seen some docmentation regarding the use of Beautiful Soup, its time to practice and put that to work! In this lab you'll formalize some of our example code into functions and scrape the lyrics from an artist of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "You will be able to:\n",
    "* Scrape Static webpages\n",
    "* Select specific elements from the DOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'urlparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-cfb9e40d760c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlxml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0murlparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_dom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'urlparse'"
     ]
    }
   ],
   "source": [
    "# From web https://stackoverflow.com/questions/1080411/retrieve-links-from-web-page-using-python-and-beautifulsoup\n",
    "import urllib\n",
    "import lxml.html\n",
    "import urlparse\n",
    "\n",
    "def get_dom(url):\n",
    "    connection = urllib.urlopen(url)\n",
    "    return lxml.html.fromstring(connection.read())\n",
    "\n",
    "def get_links(url):\n",
    "    return resolve_links((link for link in get_dom(url).xpath('//a/@href')))\n",
    "\n",
    "def guess_root(links):\n",
    "    for link in links:\n",
    "        if link.startswith('http'):\n",
    "            parsed_link = urlparse.urlparse(link)\n",
    "            scheme = parsed_link.scheme + '://'\n",
    "            netloc = parsed_link.netloc\n",
    "            return scheme + netloc\n",
    "\n",
    "def resolve_links(links):\n",
    "    root = guess_root(links)\n",
    "    for link in links:\n",
    "        if not link.startswith('http'):\n",
    "            link = urlparse.urljoin(root, link)\n",
    "        yield link  \n",
    "\n",
    "for link in get_links('http://www.google.com'):\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link Scraping\n",
    "\n",
    "Write a function to collect the links to each of the song pages from a given artist page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "#import scrapy as sc\n",
    "import fake_useragent as UserAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_url = 'https://mcstories.com/Tags/mm.html'\n",
    "\n",
    "# def grab_story_links(story_url):\n",
    "url = story_url\n",
    "html_page = requests.get(url).text\n",
    "\n",
    "soup = BeautifulSoup(html_page,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "tags = soup.find_all('a')\n",
    "tags = tags[:100]\n",
    "len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"../index.html\">The Erotic Mind-Control Story Archive</a>, <a href=\"../WhatsNew.html\">What’s New</a>, <a href=\"../Titles/index.html\">Titles</a>, <a href=\"../Authors/index.html\">Authors</a>, <a href=\"index.html\">Categories</a>, <a href=\"../ReadersPicks/index.html\">Readers’ Picks</a>, <a href=\"../FAQ.html\">FAQ</a>, <a href=\"http://www.mcgarden.org/\">The Garden of MC</a>, <a href=\"https://www.mcforum.net/index.html\">MC Forum</a>, <a class=\"mm\" href=\"../AaronNathanAndZachary/index.html\"><cite>Aaron, Nathan &amp; Zachery</cite></a>]\n"
     ]
    }
   ],
   "source": [
    "print(tags[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.mcgarden.org/',\n",
       " 'https://www.mcforum.net/index.html',\n",
       " 'http://www.mcgarden.org/',\n",
       " 'https://www.mcforum.net/index.html',\n",
       " 'https://daphnesfantasies.com/']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import lxml.html\n",
    "\n",
    "dom = lxml.html.fromstring(requests.get(story_url).content)\n",
    "\n",
    "[x for x in dom.xpath('//a/@href') if '//' in x and 'nytimes.com' not in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['../index.html'], ['../WhatsNew.html'], ['../Titles/index.html'], ['../Authors/index.html'], ['index.html'], ['../ReadersPicks/index.html'], ['../FAQ.html'], ['http://www.mcgarden.org/'], ['https://www.mcforum.net/index.html'], ['../AaronNathanAndZachary/index.html'], ['../ABCsOfDesire/index.html'], ['../Abduction/index.html'], ['../AbennaInstitute/index.html'], ['../Absinthe/index.html'], ['../AcceleratedTakedown/index.html'], ['../Acceptance/index.html'], ['../Accident/index.html'], ['../AccidentalGift/index.html'], ['../AccountantAndTheStoner/index.html'], ['../AdamAtTheArcade/index.html'], ['../AdamsChange/index.html'], ['../AddictiveSubStances/index.html'], ['../Adonis/index.html'], ['../Adrian/index.html'], ['../AdvantageSystem/index.html'], ['../HypnoMan/index.html'], ['../AdventuresOfAMadScientist/index.html'], ['../AdventuresOfAdmiralColumbiaReunion/index.html'], ['../AdyAndTheBeast/index.html'], ['../AdytumDocumentary/index.html'], ['../AfterWork/index.html'], ['../AfterWorkIntoTheFold/index.html'], ['../Aftereffects/index.html'], ['../AgainstGayMarriage/index.html'], ['../AgainstMyRuin/index.html'], ['../AgeRegression/index.html'], ['../Agreed/index.html'], ['../AidensTraining/index.html'], ['../AirplaneRide/index.html'], ['../AkiraImports/index.html'], ['../Alabaster/index.html'], ['../Aleph/index.html'], ['../AlgernonsCounselling/index.html'], ['../AllTheFunOfTheFair/index.html'], ['../AllsFair/index.html'], ['../AllureOfWellFormedBodies/index.html'], ['../AlphamanMeetsMrColeLector/index.html'], ['../Alter/index.html'], ['../AlsNewToy/index.html'], ['../AmazingAndrew/index.html'], ['../AmericanWereHypnotist/index.html'], ['../Amulet/index.html'], ['../AnalGoo/index.html'], ['../Analysis/index.html'], ['../AnatomyOfMelancholy/index.html'], ['../AndTheShipSailedOn/index.html'], ['../AnnTheSwimCoach/index.html'], ['../AnniversaryStartingOver/index.html'], ['../AnotherHypnoStoryInTheGym/index.html'], ['../AnswerMyFriendIs/index.html'], ['../ApartmentHunting/index.html'], ['../Apollyon/index.html'], ['../ApparentlyNotAllDreamsAreDreams/index.html'], ['../ApprenticesRevenge/index.html'], ['../ArabianNight/index.html'], ['../AreYouReady/index.html'], ['../Argosy/index.html'], ['../AristocratApartments/index.html'], ['../ArmyBoyToPussyslave/index.html'], ['../ArrogantLawEnforcementOfficers/index.html'], ['../ArtificialCockFlavor/index.html'], ['../AssignmentCowboy/index.html'], ['../AssignmentFuturist/index.html'], ['../AssignmentTracker/index.html'], ['../AtAnImpasse/index.html'], ['../AtTheBathhouse/index.html'], ['../AttitudeAdjustment/index.html'], ['../AuditsCanBeMoreInsightful/index.html'], ['../Awakenings/index.html'], ['../BackingIntoIt/index.html'], ['../BadGameOfPool/index.html'], ['../BadNight/index.html'], ['../BadNightForSome/index.html'], ['../BadPharma/index.html'], ['../BagBoy/index.html'], ['../BaitAndSwitch/index.html'], ['../BarbershopInduction/index.html'], ['../BarnStorm/index.html'], ['../BasicTrainingJulianObedient/index.html'], ['../BatShitCrazy/index.html'], ['../BattingAThousand/index.html'], ['../BeCarefulAsk/index.html'], ['../BeCareful/index.html'], ['../Beach/index.html'], ['../BeachMafisto/index.html'], ['../BeautyInTheBeast/index.html'], ['../BecomingHisPet/index.html'], ['../BecomingMen/index.html'], ['../BedAndBreakfast/index.html'], ['../BeforeIForget/index.html']]\n"
     ]
    }
   ],
   "source": [
    "mm_links = []\n",
    "for tag in tags:\n",
    "    mm_links.append([tag.get('href',{'class':'mm'})])\n",
    "    time.sleep(0.1)\n",
    "print(mm_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-35-658c3c3b9909>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-35-658c3c3b9909>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    link_to_get = f\"{url.replace('/Tags/mm.html','')}{str(mm_links[12]).replace(\"['..\",'')}\")\u001b[0m\n\u001b[1;37m                                                                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "link_to_get = f\"{url.replace('/Tags/mm.html','')}{str(mm_links[12]).replace(\"['..\",'')}\")\n",
    "link_to_get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append mm_Links to original url to get pages to save\n",
    "for link in mm_links:\n",
    "    \n",
    "def scrape_lyrics(song_page_url):\n",
    "    html_page = requests.get(song_page_url)\n",
    "    soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "    main_page = soup.find('div', {\"class\": \"container main-page\"})\n",
    "    main_l2 = main_page.find('div', {\"class\" : \"row\"})\n",
    "    main_l3 = main_l2.find('div', {\"class\" : \"col-xs-12 col-lg-8 text-center\"})\n",
    "    lyrics = main_l3.findAll('div')[6].text\n",
    "    return lyrics\n",
    "# For every link in the list, responses.get(url).text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print html-nested structured result\n",
    "# print(soup.prettify()[:1000])\n",
    "\n",
    "# # Get all links that have 'panic'\n",
    "# def get_links(soup,str='panic'):\n",
    "#     link_list=[]\n",
    "#     for link in soup.find_all('a'):\n",
    "#         test_link = tag.get('href',{'class':'mm'})\n",
    "\n",
    "#         if str in test_link:\n",
    "#             link_list.append(test_link)\n",
    "#         return link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use function\n",
    "# panic_links = get_links(soup,’panic’)\n",
    "\n",
    "# # Constructing reg exp to find the last 2 branches of web address (using / ... /... .html), and saves the band and song strings\n",
    "# pattern = **r**'\\\\/(?P\\<band\\\\\\w\\*)\\\\/(?P\\<song\\\\\\w\\*).html'\n",
    "# exp = re.compile(pattern) \\# the exp is a re object and can be used in methods OR functions.\n",
    "\n",
    "# # save a list of the captured band and song tokens O\n",
    "# result = []\n",
    "# [result.append(exp.findall(x)) for x in panic_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# story_tags = tags[9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for link in soup(response, parse_only=SoupStrainer('a')):\n",
    "#     if link.has_attr('href'):\n",
    "#         print(link['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern = '<tr><td><a class=\"mm\" href=\"..'\n",
    "# links = soup.find_all(text=pattern)\n",
    "# links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# string = '<tr><td><a class=\"mm\" href=\"..'\n",
    "# exp = re.compile(string)\n",
    "# found_links = exp.findall(string)\n",
    "# found_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson Below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def grab_song_links(artist_page_url):\n",
    "\n",
    "    url = artist_page_url\n",
    "\n",
    "    html_page = requests.get(url) #Make a get request to retrieve the page\n",
    "    soup = BeautifulSoup(html_page.content, 'html.parser') #Pass the page contents to beautiful soup for parsing\n",
    "\n",
    "\n",
    "    #The example from our lecture/reading\n",
    "    data = [] #Create a storage container\n",
    "\n",
    "    #Get album divs\n",
    "    albums = soup.find_all(\"div\", class_=\"album\")\n",
    "    for album_n in range(len(albums)):\n",
    "        #On the last album, we won't be able to look forward\n",
    "        if album_n == len(albums)-1:\n",
    "            cur_album = albums[album_n]\n",
    "            album_songs = cur_album.findNextSiblings('a')\n",
    "            for song in album_songs:\n",
    "                page = song.get('href')\n",
    "                title = song.text\n",
    "                album = cur_album.text\n",
    "                data.append((title, page, album))\n",
    "        else:\n",
    "            cur_album = albums[album_n]\n",
    "            next_album = albums[album_n+1]\n",
    "            saca = cur_album.findNextSiblings('a') #songs after current album\n",
    "            sbna = next_album.findPreviousSiblings('a') #songs before next album\n",
    "            album_songs = [song for song in saca if song in sbna] #album songs are those listed after the current album but before the next one!\n",
    "            for song in album_songs:\n",
    "                page = song.get('href')\n",
    "                title = song.text\n",
    "                album = cur_album.text\n",
    "                data.append((title, page, album))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Scraping\n",
    "Write a secondary function that scrapes the lyrics for each song page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remember to open up the webpage in a browser and control-click/right-click and go to inspect!\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "#Example page\n",
    "# url = 'https://www.azlyrics.com/lyrics/lilyallen/sheezus.html'\n",
    "url = \"https://www.azlyrics.com/lyrics/gomez/getmiles.html\"\n",
    "#After Inspecting the page:\n",
    "#Main DIV\n",
    "#<div>\n",
    "# <!-- Usage of azlyrics.com content by any third-party lyrics provider is prohibited by our licensing agreement. Sorry about that. -->\n",
    "# </div>\n",
    "\n",
    "html_page = requests.get(url)\n",
    "soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "soup.prettify()[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs = soup.findAll('div')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div = divs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, div in enumerate(divs):\n",
    "    if \"<!-- Usage of azlyrics.com content by any \" in div.text:\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_page = soup.find('div', {\"class\": \"container main-page\"})\n",
    "main_l2 = main_page.find('div', {\"class\" : \"row\"})\n",
    "main_l3 = main_l2.find('div', {\"class\" : \"col-xs-12 col-lg-8 text-center\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = main_l3.findAll('div')[6].text\n",
    "lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_lyrics(song_page_url):\n",
    "    html_page = requests.get(song_page_url)\n",
    "    soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "    main_page = soup.find('div', {\"class\": \"container main-page\"})\n",
    "    main_l2 = main_page.find('div', {\"class\" : \"row\"})\n",
    "    main_l3 = main_l2.find('div', {\"class\" : \"col-xs-12 col-lg-8 text-center\"})\n",
    "    lyrics = main_l3.findAll('div')[6].text\n",
    "    return lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesizing\n",
    "Create a script using your two functions above to scrape all of the song lyrics for a given artist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preview First Step\n",
    "songs = grab_song_links(\"https://www.azlyrics.com/g/gomez.html\")\n",
    "print(len(songs))\n",
    "print(songs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = grab_song_links(\"https://www.azlyrics.com/g/gomez.html\")\n",
    "url_base = \"https://www.azlyrics.com\"\n",
    "lyrics = []\n",
    "for song in songs:\n",
    "    try:\n",
    "        url_sffx = song[1].replace('..','')\n",
    "        url = url_base + url_sffx\n",
    "        lyr = scrape_lyrics(url)\n",
    "        lyrics.append(lyr)\n",
    "    except:\n",
    "        lyrics.append(\"N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(songs), len(lyrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(songs, lyrics)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Song_Name'] = df[0].map(lambda x: x[0])\n",
    "df['Song_URL_SFFX'] = df[0].map(lambda x: x[1])\n",
    "df['Album_Name'] = df[0].map(lambda x: x[2])\n",
    "df = df.rename(columns={1:'Lyrics'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing\n",
    "Generate two bar graphs to compare lyrical changes for the artist of your chose. For example, the two bar charts could compare the lyrics for two different songs or two different albums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(df.Lyrics.iloc[0].split()).value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(10,8))\n",
    "#Get top 10 words\n",
    "top10 = pd.Series(df.Lyrics.iloc[0].split()).value_counts()[:10]\n",
    "#Plot as bar graph\n",
    "top10.plot(ax=axes[0], kind='barh')\n",
    "#Add Subplot Title\n",
    "axes[0].set_title('Top 10 Lyrics for {}'.format(df['Song_Name'].iloc[0]))\n",
    "#Repeat\n",
    "#Get top 10 words\n",
    "top10 = pd.Series(df.Lyrics.iloc[1].split()).value_counts()[:10]\n",
    "#Plot as bar graph\n",
    "top10.plot(ax=axes[1], kind='barh')\n",
    "#Add Subplot Title\n",
    "axes[1].set_title('Top 10 Lyrics for {}'.format(df['Song_Name'].iloc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level - Up\n",
    "\n",
    "Think about how you structured the data from your web scraper. Did you scrape the entire song lyrics verbatim? Did you simply store the words and their frequency counts, or did you do something else entirely? List out a few different options for how you could have stored this data. What are advantages and disadvantages of each? Be specific and think about what sort of analyses each representation would lend itself to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Response: \n",
    "\n",
    "\n",
    "Currently the above function scrapes the lyrics verbatim. This costs the most in terms of storage but is the most malleable for future analysis. Alternative views such as a dictionary count of word frequencies would save storage space and makes some analyses quicker (such as plotting word frequencies) but would make other analyses such as a n-gram analysis impossible. In this context, scraping raw transcripts and then producing additional cached views of summaries such as frequency is probably the most sensible as storage size is not likely to be an issue at this scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Congratulations! You've now practiced your Beautiful Soup knowledge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env-ext",
   "language": "python",
   "name": "learn-env-ext"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
